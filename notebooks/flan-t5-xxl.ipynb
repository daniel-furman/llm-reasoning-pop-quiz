{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e740b4d-aa46-4f9e-b860-81ae60534ac3",
     "showTitle": false,
     "title": ""
    },
    "id": "i5m7HwtsrNex"
   },
   "source": [
    "# LLM reasoning pop quiz\n",
    "\n",
    "Do open-sourced LLMs have the reasoning prowess of their closed-sourced siblings?\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/LLM-reasoning-pop-quiz/blob/main/notebooks/flan-t5-xxl.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Setup\n",
    "2. Read yaml config\n",
    "3. Load model\n",
    "4. Run the quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fc7220-82ea-45f4-b8f2-febf499b1d64",
     "showTitle": false,
     "title": ""
    },
    "id": "bz3OEcHXrNey"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd369caa-4965-4710-9611-b4b96da5c244",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAx4WnwarNey",
    "outputId": "8d7a9d9d-5d72-41bf-aeef-ce9ee8f8cc72"
   },
   "outputs": [],
   "source": [
    "# detailed information on the GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0Gh4WEcED95",
    "outputId": "d0c59da8-1505-4f6a-bfa6-68d5c77d1234"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/daniel-furman/LLM-reasoning-pop-quiz.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdZGBvF_Jrxo",
    "outputId": "4fc74a49-ce38-47a4-bfd2-4cede5584447"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yigQDMrGGV1S",
    "outputId": "ffb60d15-5ecf-4120-b5ae-9b07a69c248c"
   },
   "outputs": [],
   "source": [
    "# install necessary libraries\n",
    "import os\n",
    "\n",
    "os.chdir(\"/content/LLM-reasoning-pop-quiz\")\n",
    "!pip install -q -U -r requirements.txt\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c585315-960c-45ae-8f17-eee6c066e1f0",
     "showTitle": false,
     "title": ""
    },
    "id": "fCN8gVnErNe0"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "# import helpers\n",
    "\n",
    "from drf_llm_boilers import llm_boiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glqF1V1RXgYp"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "\n",
    "transformers.set_seed(4129408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d59cadf-9266-4d7e-986d-a0d3cc7ea62a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjHuOa_JrNe0",
    "outputId": "1c23c9fb-c9ba-488c-b791-75ff8edb924b"
   },
   "outputs": [],
   "source": [
    "# print GPU available memory\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "max_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGyCV_6PED97"
   },
   "source": [
    "## Read in the yaml config for the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQtDrkjEED97",
    "outputId": "cd73c2d9-19f5-4ab8-93fc-4d6ec9d60f05"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/LLM-reasoning-pop-quiz/configs/pop_quiz.yml\", \"r\") as file:\n",
    "    pop_quiz = yaml.safe_load(file)\n",
    "pop_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d69770-8ac6-417f-93ec-5a04e7624f24",
     "showTitle": false,
     "title": ""
    },
    "id": "Rl3EGKoXrNe1"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "8d2ae5ddd79a47a586bc995148ed9556",
      "444acf645ae440c9817207b5c423d37c",
      "75edb5128fb64966842622f3c4687ea4",
      "1ec72641e80247e1b6962b688dc1ef48",
      "190f5580628f4de7a7fcdbbdc7f9e2e7",
      "76208b1e0b5445c99cc39ba3b245ba33",
      "a0d05783713249b1a1e6d0cc25f758cb",
      "9719c4a8198e411aa5331f61b3496130",
      "8493fb11960a49b4b905fc6e4980e654",
      "4837f2f1426e4252b2a2e9a2cbcd642c",
      "5bd98edf23f64d74b63b0bb35a592171"
     ]
    },
    "id": "wJEHXeDeI0zU",
    "outputId": "41436164-ca1a-4ccd-f94a-46df41cbec49"
   },
   "outputs": [],
   "source": [
    "# load google/flan-t5-xxl\n",
    "# see source: https://huggingface.co/google/flan-t5-xxl#usage\n",
    "\n",
    "# this cell will take a long time, to avoid: deploy the LLM as an API inference endpoint\n",
    "\n",
    "model_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "model = llm_boiler(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7XJRdkMMCPR",
    "outputId": "0e72fb43-583a-411a-ea35-83ade94d2a99"
   },
   "outputs": [],
   "source": [
    "print(model.name, \"\\n\")\n",
    "print(model.tokenizer, \"\\n\")\n",
    "print(model.model, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb267c9-c76b-4d56-a411-483841bb787a",
     "showTitle": false,
     "title": ""
    },
    "id": "6MxLSoPXrNe2"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "* For text generation options, refer to [https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline)\n",
    "* Below prompts are borrowed from [https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhQuAUHOrNM5"
   },
   "source": [
    "### Example 1: Zero-shot reasoning conditioned on good performance\n",
    "* From https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pR8hmCkNF_U",
    "outputId": "02bc9bed-187a-4d0b-8a78-00da188f9081"
   },
   "outputs": [],
   "source": [
    "# run zero shot questions\n",
    "\n",
    "for itr, prompt in enumerate(pop_quiz[\"prompts\"][\"zero_shot\"]):\n",
    "    print(f\"Question 1.{itr+1}\")\n",
    "    print(f'Prompt: \"{prompt}\"\\n')\n",
    "    start_time = time.time()\n",
    "    generated_text = model.run(\n",
    "        prompt=prompt,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=1.0,\n",
    "        do_sample=True,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generations: \"{generated_text}\"\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VklIoMqnrmKa"
   },
   "source": [
    "### Example 2: Chain-of-thought reasoning with few-shot examples\n",
    "* From https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vHxpU-gOVjr",
    "outputId": "73d270b0-1e33-487f-b402-927456b2acb2"
   },
   "outputs": [],
   "source": [
    "# run cot few-shot questions\n",
    "\n",
    "for itr, prompt in enumerate(pop_quiz[\"prompts\"][\"cot_few_shot\"]):\n",
    "    print(f\"Question 2.{itr+1}\")\n",
    "    print(f'Prompt: \"{prompt}\"\\n')\n",
    "    start_time = time.time()\n",
    "    generated_text = model.run(\n",
    "        prompt=prompt,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generations: \"{generated_text}\"\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkgbWXQRrv4h"
   },
   "source": [
    "### Example 3: Least to most prompting\n",
    "* From https://arxiv.org/abs/2205.10625\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzkHy6-_O00Y",
    "outputId": "b0bf24bc-b66a-4fae-e3a9-53e4787bcbfa"
   },
   "outputs": [],
   "source": [
    "# run least to most questions\n",
    "\n",
    "for itr, prompts in enumerate(pop_quiz[\"prompts\"][\"least_to_most\"]):\n",
    "    print(f\"Question 3.{itr+1}\")\n",
    "    # Start with sub question #1\n",
    "    sub_question_1 = prompts[0]\n",
    "    print(f'Prompt: \"{sub_question_1}\"\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    res_1 = model.run(\n",
    "        prompt=sub_question_1,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generation: \"{res_1}\"\\n')\n",
    "\n",
    "    # Now do sub question #2 by appending answer to sub question #1\n",
    "    sub_question_2 = f\"{sub_question_1} {res_1} {prompts[1]}\"\n",
    "    print(f'Prompt: \"{sub_question_2}\"\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    res_2 = model.run(\n",
    "        prompt=sub_question_2,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generation: \"{res_2}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No9YFIU-lnjX"
   },
   "source": [
    "### Example 4: Tab-CoT\n",
    "\n",
    "* See https://arxiv.org/abs/2305.17812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyEe9za5E6oE",
    "outputId": "c6decb48-28a6-41e0-8e07-30ea7d218dd9"
   },
   "outputs": [],
   "source": [
    "# run tab-cot questions\n",
    "\n",
    "for itr, prompts in enumerate(pop_quiz[\"prompts\"][\"tab_cot\"]):\n",
    "    print(f\"Question 4.{itr+1}\")\n",
    "    # Start with sub question #1\n",
    "    sub_question_1 = prompts[0]\n",
    "    print(f'Prompt: \"{sub_question_1}\"\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    res_1 = model.run(\n",
    "        prompt=sub_question_1,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generation: \"{res_1}\"\\n')\n",
    "\n",
    "    # Now do sub question #2 by appending answer to sub question #1\n",
    "    sub_question_2 = f\"{sub_question_1} {res_1} {prompts[1]}\"\n",
    "    print(f'Prompt: \"{sub_question_2}\"\\n')\n",
    "\n",
    "    start_time = time.time()\n",
    "    res_2 = model.run(\n",
    "        prompt=sub_question_2,\n",
    "        eos_token_ids=model.tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"\\n\")\n",
    "    print(f'Text generation: \"{res_2}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB7Zi75xP6v3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hf_LLM_load_and_run_example_dolly7b",
   "widgets": {}
  },
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "190f5580628f4de7a7fcdbbdc7f9e2e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ec72641e80247e1b6962b688dc1ef48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4837f2f1426e4252b2a2e9a2cbcd642c",
      "placeholder": "​",
      "style": "IPY_MODEL_5bd98edf23f64d74b63b0bb35a592171",
      "value": " 5/5 [03:12&lt;00:00, 36.00s/it]"
     }
    },
    "444acf645ae440c9817207b5c423d37c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76208b1e0b5445c99cc39ba3b245ba33",
      "placeholder": "​",
      "style": "IPY_MODEL_a0d05783713249b1a1e6d0cc25f758cb",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "4837f2f1426e4252b2a2e9a2cbcd642c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bd98edf23f64d74b63b0bb35a592171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75edb5128fb64966842622f3c4687ea4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9719c4a8198e411aa5331f61b3496130",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8493fb11960a49b4b905fc6e4980e654",
      "value": 5
     }
    },
    "76208b1e0b5445c99cc39ba3b245ba33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8493fb11960a49b4b905fc6e4980e654": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d2ae5ddd79a47a586bc995148ed9556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_444acf645ae440c9817207b5c423d37c",
       "IPY_MODEL_75edb5128fb64966842622f3c4687ea4",
       "IPY_MODEL_1ec72641e80247e1b6962b688dc1ef48"
      ],
      "layout": "IPY_MODEL_190f5580628f4de7a7fcdbbdc7f9e2e7"
     }
    },
    "9719c4a8198e411aa5331f61b3496130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d05783713249b1a1e6d0cc25f758cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
